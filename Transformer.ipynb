{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1JOwcnDJYrFF2Kqqe4SDWO9s62d37xxvW","authorship_tag":"ABX9TyMfNftkSlWUHTlNK+sW7Sfn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["def get_device():\n","  return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"SaU0banIMk8U","executionInfo":{"status":"ok","timestamp":1723127411769,"user_tz":-330,"elapsed":3,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6bjfqw6qLZ4v","executionInfo":{"status":"ok","timestamp":1723127423621,"user_tz":-330,"elapsed":11854,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import math\n","from torch import nn\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import math\n","\n","def scaled_dot_prod(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","\n","    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n","\n","    if mask is not None:\n","\n","        scaled = scaled.masked_fill(mask == 0, float('-inf'))\n","\n","    attention = F.softmax(scaled, dim=-1)\n","    values = torch.matmul(attention, v)\n","\n","    return values, attention\n"],"metadata":{"id":"04_27W4PMyQb","executionInfo":{"status":"ok","timestamp":1723127423621,"user_tz":-330,"elapsed":15,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class PosEnc(nn.Module):\n","  def __init__(self, d_model, max_seq_len):\n","     super().__init__()\n","     self.max_seq_len = max_seq_len\n","     self.d_model = d_model\n","\n","  def forward(self):\n","    even_i = torch.arrange(0,self.d_model,2).float()\n","    denom = torch.pow(10000, even_i/self.d_model)\n","    pos = (torch.arrange(self.max_seq_len)).reshape(self.max_seq_len,1)\n","    even_PE = torch.sin(pos/denom)\n","    odd_PE = torch.cos(pos/denom)\n","    stacked = torch.stack([even_PE, odd_PE], dim = 2)\n","    PE = torch.flatten(stacked,start_dim= 1, end_dim= 2)\n","    return PE"],"metadata":{"id":"_hXUltfXNmWq","executionInfo":{"status":"ok","timestamp":1723127423621,"user_tz":-330,"elapsed":14,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class SentEmb(nn.Module):\n","  def __init(self,max_seq_len, d_model, lang_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n","    super.__init__()\n","    self.vocab_seize = len(lang_to_index)\n","    self.max_seq_len = max_seq_len\n","    self.embedding = nn.Embedding(self.vocab_size, d_model)\n","    self.lang_to_index = lang_to_index\n","    self.positional_encoder = PosEnc(d_model, max_seq_len)\n","    self.dropout = nn.Dropout(p=0.1)\n","    self.START_TOKEN = START_TOKEN\n","    self.END_TOKEN = END_TOKEN\n","    self.PADDING_TOKEN = PADDING_TOKEN\n","\n","  def batch_tknz(self,batch,start_token,end_token):\n","    def tknz(sent, start_token, end_token):\n","      sent_word_indicies = [ self.lang_to_index[token] for token in list(sent)]\n","      if start_token:\n","        sent_word_indicies.insert(0,self.lang_to_index[self.START_TOKEN])\n","      if end_token:\n","        sent_word_indicies.append(self.lang_to_index[self.END_TOKEN])\n","      for _ in range(len(sent_word_indicies), self.max_seq_len):\n","        sent_word_indicies.append(self.lang_to_index[self.PADDING_TOKEN])\n","      return torch.tensor(sent_word_indicies)\n","\n","\n","    tokenized = []\n","    for sent_num in range(len(batch)):\n","      tokenized.append(tknz(batch[sent_num], start_token,end_token))\n","    tokenized = torch.stack(tokenized)\n","    return tokenized.to(get_device())\n","\n","\n","  def forward(self, x, start_token, end_token):\n","    x = self.batch_tokenize(x, start_token, end_token)\n","    x = self.embedding(x)\n","    pos = self.positional_encoder().to(get_device())\n","    x = self.dropout(x + pos)\n","\n","    return x\n"],"metadata":{"id":"rR-6Bh1YPERv","executionInfo":{"status":"ok","timestamp":1723127423621,"user_tz":-330,"elapsed":14,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class multheadatt(nn.Module):\n","  def __init__(self,d_model,num_heads):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.num_heads = num_heads\n","    self.head_dim = d_model//num_heads\n","    self.qkv_layer = nn.Linear(d_model, 3*d_model)\n","    self.linear_layer = nn.Linear(d_model, d_model)\n","\n","\n","  def forward( self, x, mask):\n","    batch_size, seq_len, d_model = x.size()\n","    qkv = self.qkv_layer(x)\n","    qkv = qkv.reshape(batch_size,seq_len,self.num_heads, 3*self.head_dim)\n","    qkc = qkv.permute(0,2,1,3)\n","    q,k,v = qkv.chunk(3,dim=-1)\n","    values, attention = scaled_dot_prod(q,k,v,mask)\n","    values = values.permute(0,2,1,3).reshape(batch_size, seq_len,self.num_heads, self.head_dim)\n","    out = self.linear_layer(values)\n","\n","    return out"],"metadata":{"id":"dPFae9vsZFbi","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":15,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","  def __init__(self, parameters_shape, eps = 1e-5):\n","    super().__init()\n","    self.paramenters_shape = parameters_shape\n","    self.eps = eps\n","    self.gamma = nn.Parameter(torch.ones(parameters_shape))\n","    self.beta = nn.Parameter(torch.zeros(parameters_shape))\n","\n","\n","  def forward(self,inputs):\n","    dims = [-(i + 1) for i in range (len(self.paramenters_shape))]\n","    mean = inputs.mean(dim = dims, keepdim = True)\n","    var = ((inputs - mean)** 2).mean(dims + dims)\n","    std = (var + self.eps).sqrt()\n","    y = (inputs - mean)/ std\n","    out = self.gamma * y + self.beta\n","    return out"],"metadata":{"id":"0WfJ1BhmPo5d","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":15,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class PosFF(nn.Module):\n","  def __init__ (self, d_model, hidden, drop_prob = 0.1):\n","    super(PosFF, self). __init()\n","    self.linear1 = nn.Linear(d_model, hidden)\n","    self.linear1 = nn.Linear(d_model, d_model)\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.Dropout(p = drop_prob)\n","\n","\n","  def forward(self, x):\n","    x = self.linear1(x)\n","    x = self.relu(x)\n","    x = self.dropout(x)\n","    x = self.linear2(x)\n","\n","    return x\n"],"metadata":{"id":"8MNi7XwiZRNn","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":15,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nVZ_ZDy673T7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n","    super(EncoderLayer, self). _init__()\n","    self.attention = multheadatt(d_model = d_model, num_heads=num_heads)\n","    self.norm1 = LayerNorm(parameters_shape=[d_model])\n","    self.dropout1 = nn.Dropout(p = drop_prob)\n","    self.ffn = PosFF(d_model=d_model, hidden = ffn_hidden, drop_prob=drop_prob)\n","    self.norm2 = LayerNorm(parameters_shape=[d_model])\n","    self.dropout2 = nn.Dropout(p = drop_prob)\n","\n","\n","\n","  def forward(self, x, self_attention_mask):\n","    residual_x = x.clone()\n","    x = self.attention(x, mask = self_attention_mask)\n","    x = self.dropout1(x)\n","    x = self.norm1(x + residual_x)\n","    residual_x = x.clone()\n","    x = self.ffn(x)\n","    x = self.dropout2(x)\n","    x = self.norm2(x + residual_x)\n","    return x\n"],"metadata":{"id":"Jq4fz_zidBD2","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":15,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class SeqEnc(nn.Sequential):\n","  def forward(self, inputs):\n","    x,self_attention_mask = inputs\n","    for module in self._modules.values():\n","      x = module(x, self_attention_mask)\n","    return x"],"metadata":{"id":"p64wIxDAMpo1","executionInfo":{"status":"ok","timestamp":1723107460763,"user_tz":-330,"elapsed":441,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","  def __init__(self, d_model, ffn_head, num_heads,drop_prob,num_layers,max_seq_len, lang_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n","    super().__init__()\n","    self.sentence_embedding = SentEmb(max_seq_len,d_model, lang_to_index,START_TOKEN, END_TOKEN, PADDING_TOKEN)\n","    self.layers = SeqEnc(*[EncoderLayer(d_model, ffn_head, num_heads, drop_prob)\n","                                      for _ in range(num_layers)])\n","\n","\n","    def forward(self,x,self_attention_mask, start_token, end_token):\n","      x = self.sentence_embedding(x, start_token, end_token)\n","      x = self.layers(x, self_attention_mask)\n","\n","\n","      return x\n","\n","\n",""],"metadata":{"id":"seb1ukT3NUIU","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":14,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class multheadcrossatt(nn.Module):\n","\n","  def __init__(self,d_model,num_heads):\n","    super.__init__()\n","    self.d_model = d_model\n","    self.num_heads = num_heads\n","    self.head_dim = d_model // num_heads\n","    self.kv_layer = nn.linear(d_model, 2 * d_model)\n","    self.q_layer = nn.Linear(d_model, d_model)\n","    self.linear_layer = nn.Linear(d_model, d_model)\n","\n","\n","  def forward(self, x, y, mask):\n","    batch_size, seq_len, d_model = x.size()\n","    kv = self.kv_layer(x)\n","    q = self.q_layer(y)\n","\n","    kv = kv.reshape(batch_size, seq_len, self.num_heads, 2 * self.head_dim)\n","    q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n","\n","    kv = kv.permute(0,2,1,3)\n","    q = q.permute(0,2,1,3)\n","    k,v = kv.chunk(2, dim = -1)\n","\n","    values, attention = scaled_dot_prod(q,k,v,mask)\n","    values = values.permute(0,2,1,3).reshape(batch_size, seq_len, d_model)\n","\n","    out = self.linear_layer(values)\n","\n","    return out"],"metadata":{"id":"HM-ksCvqPgvN","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":14,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n","    super(DecoderLayer, self).__init__()\n","    self.self_attention = multheadatt(d_model = d_model, num_heads = num_heads)\n","    self.layer_norm1 = LayerNorm(parameters_shape=[d_model])\n","    self.dropout = nn.dropout( p= drop_prob)\n","\n","\n","    self.enc_dec_att = multheadcrossatt( d_model=d_model, num_heads=num_heads)\n","    self.layer_norm2 = LayerNorm(parameters_shape=[d_model])\n","    self.droput2 = nn.dropout(p = drop_prob)\n","\n","\n","    self.ffn = PosFF(d_model = d_model, num_heads = num_heads)\n","    self.layer_norm3 = LayerNorm(parameters_shape=[d_model])\n","    self.dropout3 = nn.dropout(p = drop_prob)\n","\n","  def forward(self,x,y,self_attention_mask,cross_attention_mask):\n","    _y = y.clone()\n","    y = self.self_attention(y, mask = self_attention_mask)\n","    y = self.dropout1(y)\n","    y = self.layer_norm1(y +_y)\n","\n","\n","    _y = y.clone()\n","    y = self.enc_dec_att(x,y, mask=cross_attention_mask)\n","    y = self.droput2(y)\n","    y = self.layer_norm2(y + _y)\n","\n","\n","    _y = y.clone()\n","    y = y.self.ffn(y)\n","    y = self.dropout3(y)\n","    y = self.layer_norm3(y + _y)\n","\n","    return y\n"],"metadata":{"id":"kZgIefIQTP_f","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":14,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class seqdec(nn.Sequential):\n","  def forward(self, *inputs):\n","    x,y,self_attention_mask,cross_attention_mask = inputs\n","    for module in self._modules.values():\n","      y = module(x,y, self_attention_mask,cross_attention_mask)\n","\n","    return y\n",""],"metadata":{"id":"k3qc1W7sasFK","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":14,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class decoder(nn.Module):\n","  def __init__(self,d_model,ffn_hidden,num_heads,head_dim,drop_prob, num_layers, max_seq_len,lang_to_index, START_TOKEN,END_TOKEN,PADDING_TOKEN):\n","     super().__init__()\n","     self.sentence_embedding = SentEmb(max_seq_len, d_model, lang_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n","     self.layers = seqdec(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n","     for _ in range (num_layers)])\n","\n","  def forward(self, x,y,self_attention_mask,cross_attention_mask,start_token, end_token):\n","    y = self.sentence_embedding(y, start_token, end_token)\n","    y = self.layers(x,y, self_attention_mask, cross_attention_mask)\n","    return y\n"],"metadata":{"id":"wczA8_Pubke7","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":13,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn. Module):\n","  def __init__(self,d_model,ffn_hidden, num_heads, drop_prob,num_layers, max_seq_len,fr_vocab_size,eng_vocab_size, eng_2_index, fr_2_index,START_TOKEN, END_TOKEN, PADDING_TOKEN):\n","    super().__init__()\n","    self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_seq_len,eng_2_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n","    self.decoder = decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_seq_len,fr_2_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n","    self.linear = nn.Linear(d_model, fr_vocab_size)\n","    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","\n","\n","  def forward(self,x,y,enc_self_att_mask=None, dec_self_att_mask = None, dec_cross_att_mask = None, enc_start_token = False, enc_end_token = False, dec_start_token = False, dec_end_token = False):\n","    x = self.encoder(x, enc_self_att_mask, start_token = enc_start_token, end_token = enc_end_token)\n","    out = self.decoder(x,y,dec_self_att_mask,dec_cross_att_mask, start_token = dec_start_token, end_token = dec_end_token)\n","    out = self.linear(out)\n","\n","    return out"],"metadata":{"id":"HfMJXysZcMEs","executionInfo":{"status":"ok","timestamp":1723127423622,"user_tz":-330,"elapsed":14,"user":{"displayName":"sangharsh verma","userId":"15615137873558791752"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e3Ho1tawoSOx"},"execution_count":null,"outputs":[]}]}